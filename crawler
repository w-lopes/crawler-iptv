#!/usr/bin/env python3
# -*- coding: Utf-8 -*-
import datetime
import inspect
import os
import re
import requests
import sys
import threading
import validators

class Crawler:
	"""
	Classe para 'crawlear' os sites.
	"""

	links = [
		"listasiptvgratis"
	]

	file_name_m3u     = "bergamota.m3u"
	file_name_m3u_tmp = "bergamota.tmp"

	def listasiptvgratis(self):
		"""
		Crawling para o site 'https://listasiptvgratis.com/'.

		Returns:
			str[]: Links coletados do site
		"""
		content = self._getLinkLines("https://listasiptvgratis.com/", True)
		links   = []
		valids  = [i for j in (range(21, 31), range(84, 94)) for i in j]
		counter = 0
		for line in content:
			if "data-clipboard-text" not in line:
				continue
			counter += 1
			if counter not in valids:
				continue
			links.append(re.sub("\".*", "", re.sub(".*data-clipboard-text=\"", "", line)))
		return links


	def _getLinkLines(self, link, split=False):
		"""
		Retorna o conteudo de uma link splitado por quebra de linha.

		Args:
			link (str): Link para coletar o conteudo

		Returns:
			str[]: Linhas coletadas
		"""
		headers  = {
			"User-Agent" : "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2227.1 Safari/537.36",
			"Connection" : "close"
		}
		try:
			sess     = requests.Session()
			response = sess.get(link, headers=headers, timeout=5)
			sess.close()
			del sess
			if split:
				return re.sub("\r", "", response.text).split("\n")
			else:
				return re.sub("\r", "", response.text)
		except:
			return []


	def _getExtInfo(self, line, info):
		return re.sub("\".*", "", re.sub(".*" + info + "=\"", "", line))


	def _threadParseList(self, link):
		print("Downloading list:", link)
		lines = self._getLinkLines(link)
		if len(lines) > 1 * 1024 * 1024:
			print("\t[\033[91mSIZE\033[0m]", link)
			return
		if lines:
			re.sub("(#EXTINF.*\n)(http.*\n)", self._regExCallback, lines)

	def clearFile(self):
		path = os.path.dirname(os.path.abspath(inspect.stack()[0][1]))
		with open(os.path.join(path, self.file_name_m3u_tmp), "w+") as file:
			file.write("")


	def _regExCallback(self, match):
		global all_urls, all_names, black_list, beg_with

		all      = match.group(0).strip().split("\n")
		line     = all[0]
		url      = all[1]
		headers  = {
			"User-Agent"    : "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2227.1 Safari/537.36",
			"Connection"    : "close",
			"Cache-Control" : "no-cache"
		}

		logo  = self._getExtInfo(line, "tvg-logo").strip()
		name  = self._getExtInfo(line, "tvg-name").strip()
		aname = re.sub("\W", "", name)

		name  = name if len(name) and "#" not in name else re.sub(".*,", "", line).strip()
		logo  = logo if validators.url(logo) else ""
		group = ""

		if re.match("FOX\s\d+", name):
			return

		for i in black_list:
			if i in name.lower() or i in url.lower():
				return

		for attr, value in beg_with.items():
			if name.lower().startswith(attr):
				group = value
				break

		if not group:
			return;

		if (url in all_urls) or (aname in all_names and all_names[aname] > 1):
			print("\t[\033[93mDUP\033[0m]", name)
			return

		if not validators.url(url):
			return

		try:
			sess   = requests.Session()
			req    = sess.get(url, headers=headers, stream=True, timeout=3)
			status = req.status_code
			sess.close()
			del sess, req
		except:
			print("\t[\033[91mOFF\033[0m] (Timeout)", name)
			return

		if ".m3u" not in url and ".ts" not in url:
			return

		if status > 400:
			print("\t[\033[91mOFF\033[0m][", status, "]: ", name)
			return

		all_urls.append(url)
		if (name in all_names):
			all_names[aname] = all_names[aname] + 1
		else:
			all_names[aname] = 1

		print("\t[\033[92mON\033[0m] \033[1;30m(", group, ")\033[0m[", status, "]: ", name)
		self.appendSourceLine(name, logo, group, url)


	def appendLine(self, line):
		line = line + "\n\n"
		path = os.path.dirname(os.path.abspath(inspect.stack()[0][1]))

		while global_lock.locked():
			continue

		global_lock.acquire()

		with open(os.path.join(path, self.file_name_m3u_tmp), "a+") as file:
			file.write(line)
			file.close()

		global_lock.release()
		return True


	def appendSourceLine(self, name, logo, group, url):
		line = "#EXTINF:-1 tvg-name=\"%s\" tvg-logo=\"%s\" group-title=\"%s\",%s\n%s" % (
			name,
			logo,
			group,
			name.upper(),
			url
		)
		return self.appendLine(line)


if __name__ == "__main__":

	black_list = [
		"bergamota",
		"canales",
		"chile",
		"infantiles",
		"peliculas",
		"ecuador",
		"mexico",
		"calidad",
		"espan",
		"españ",
		"nuestra",
		"camara",
		"câmara",
		"latino",
		"suiza",
		"collage",
		"usa",
		"sur",
		"business",
		"atlanta",
		"sexta",
		"spain",
		"privada",
		"inernacional",
		"classic",
		"fox comedy",
		"fox premium",
		"fox action",
		"fox family",
		"fox cinema",
		"fox 23",
		"fox 5",
		"estados unidos",
        "premiun",
		"manipuladora",
		"sbt pi",
		"interior",
		"no audio",
		"noaudio",
		"radio",
		"mtv rocks",
		"mtv dance",
		"mtv live",
		"mtv hits",
		"japan",
		"russia",
		"uk",
        "globo sp",
        "globo rj",
        "globo minas",
        "globo pi",
        "sbt goiás",
        "sbt goias",
        "sbt recife",
        "fm",
        "internacion",
        "sbt ba",
        "sbt mt",
        "sbt ma",
        "sbt pe",
        "kosova",
        "clipes",
        "band pa",
        "banda max",
        "globo brasil",
        "record mt",
        "mt",
        "sc",
        "nordeste",
        "goias",
        "portugal",
        "água boa",
        "agua boa",
        "sbt go",
        "record pt",
        "signature",
        "angola"
	]
	beg_with = {
		"sportv"    : "Esportes",
		"fox sport" : "Esportes",
		"foxsport"  : "Esportes",
		"combate"   : "Esportes",
		"espn"      : "Esportes",
		"premiere"  : "Esportes",
		"esporte"   : "Esportes",

		"telecine"         : "Filmes",
		"megapix"          : "Filmes",
		"hbo"              : "Filmes",
		"studio universal" : "Filmes",
		"space"            : "Filmes",
		"max"              : "Filmes",
		"paramount"        : "Filmes",
		"band sport"       : "Filmes",

		"universal" : "Séries",
		"fox"       : "Séries",
		"fx"        : "Séries",
		"warner"    : "Séries",
		"sony"      : "Séries",
		"syfy"      : "Séries",
		"tnt"       : "Séries",
		"axn"       : "Séries",
		"amc"       : "Séries",
		"a&e"       : "Séries",
		"tbs"       : "Séries",

		"cartoon"        : "Infantis",
		"nick"           : "Infantis",
		"disney"         : "Infantis",
		"gloob"          : "Infantis",
		"natgeokids"     : "Infantis",
		"natgeo kids"    : "Infantis",
		"nat geo kids"   : "Infantis",
		"tooncast"       : "Infantis",
		"discovery kids" : "Infantis",
		"boomerang"      : "Infantis",

		"discovery"           : "Documentários",
		"natgeo"              : "Documentários",
		"national geographic" : "Documentários",
		"animal planet"       : "Documentários",
		"history"             : "Documentários",
		"h2"                  : "Documentários",
		"investigacao"        : "Documentários",
		"investigação"        : "Documentários",
		"home & health"       : "Documentários",
		"home&health"         : "Documentários",
		"h&h"                 : "Documentários",

		"gnt"            : "Variedades",
		"mtv"            : "Variedades",
		"multishow"      : "Variedades",
		"comedy central" : "Variedades",
		"tru tv"         : "Variedades",
		"trutv"          : "Variedades",

		"globo news"   : "Notícias",
		"band news"    : "Notícias",
		"record news"  : "Notícias",

		"globo"   : "Abertos",
		"sbt"     : "Abertos",
		"record"  : "Abertos",
		"band"    : "Abertos",
		"futura"  : "Abertos",
		"cultura" : "Abertos",

		# "playboy"  : "Adultos +18",
		# "sexyhot"  : "Adultos +18",
		# "venus"    : "Adultos +18",
		# "miami tv" : "Adultos +18",
		# "miamitv"  : "Adultos +18",
		# "jasmine"  : "Adultos +18"
	}

	all_urls  = []
	all_names = {}

	try:
		global_lock = threading.Lock()
		links       = Crawler()
		threads     = []
		now         = datetime.datetime.now()
		dthr        = now.strftime("%d/%m/%Y %H:%M:%S")
		path        = os.path.dirname(os.path.abspath(inspect.stack()[0][1]))
		all         = [
			"https://bit.ly/faustinotv",
			"https://tinyurl.com/CANAISGLAU",
			"https://tinyurl.com/ESPORTEGLAU",
			"https://tinyurl.com/DOCUMENTARIOSGLAU",
			"http://minhalista.live/LANNNN",
			"http://bit.ly/Will-Canais"
		]

		links.clearFile()

		links.appendLine("#EXTM3U")
		links.appendLine("#PLAYLISTV: pltv-name=\"Bergamota list\" pltv-description=\"Lista de canais muito cítrica :)\" pltv-author=\"Bergamota Inc.\"")
		links.appendLine("#EXTINF:-1 tvg-id="" group-title=\"Info\", Atualizada em: " + dthr)

		for i in links.links:
			print("Collecting from", i)
			crawl = getattr(links, i)
			all  += crawl()

		all = list(set(all))

		for link in all:
			t = threading.Thread(target=links._threadParseList, args=(link,))
			t.start()
			threads.append(t)

		for t in threads:
			t.join()

		os.rename(os.path.join(path, links.file_name_m3u_tmp), os.path.join(path, links.file_name_m3u))
		print("Done")
	except (KeyboardInterrupt, SystemExit):
		os.rename(os.path.join(path, links.file_name_m3u_tmp), os.path.join(path, links.file_name_m3u))
		print("Exiting")
		sys.exit(0)
	except Exception as e:
		print("Exception", e)
		pass
