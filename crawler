#!/usr/bin/env python3
# -*- coding: Utf-8 -*-
import datetime
import inspect
import os
import re
import requests
import sys
import threading
import time
import validators

from googleapiclient.discovery import build

class Crawler:

    links = [
        "minhalistalive",
        "googlepastebin",
        "listasiptvgratis"
    ]

    file_name_m3u = "bergamota.m3u"
    file_name_m3u_tmp = "bergamota.tmp"

    def minhalistalive(self):
        links  = []
        page   = self._getLinkLines("https://minhalista.live/listas-iptv/mais-populares", True, 60)
        page  += self._getLinkLines("https://minhalista.live/listas-iptv/atualizadas", True, 60)
        for i in range(2, 5):
            page  += self._getLinkLines("https://minhalista.live/listas-iptv/atualizadas?page=%s" % (i), True, 60)
        for line in page:
            if "btn-primary" not in line:
                continue
            links.append(re.sub("(.*href=\")|(listas-iptv\/)|(\"\sclass.*)", "", line))
        return links

    def googlepastebin(self):
        links = []
        service = build("customsearch", "v1", developerKey="AIzaSyC1uUnvc3dMHgM_JbCxOM5Pj0lU7moxFo4")
        res = service.cse().list(
            q=["extm3u", "brasil"],
            cx="013227457599740062995:m4jg7xrozow",
            sort="date"
        ).execute()

        for result in res["items"]:
            links.append(re.sub(".com/", ".com/raw/", result["formattedUrl"]))
        return links

    def listasiptvgratis(self):
        content = self._getLinkLines("https://listasiptvgratis.com/", True)
        links   = []
        valids  = [i for j in (range(21, 31), range(84, 94)) for i in j]
        counter = 0
        for line in content:
            if "data-clipboard-text" not in line:
                continue
            counter += 1
            if counter not in valids:
                continue
            links.append(re.sub("\".*", "", re.sub(".*data-clipboard-text=\"", "", line)))
        return links


    def _getLinkLines(self, link, split=False, timeout=5):
        headers  = {
            "User-Agent" : "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2227.1 Safari/537.36",
            "Connection" : "close"
        }
        try:
            sess     = requests.Session()
            response = sess.get(link, headers=headers, timeout=timeout)
            sess.close()
            del sess
            if split:
                return re.sub("\r", "", response.text).split("\n")
            else:
                return re.sub("\r", "", response.text)
        except:
            print("Fail to collect from", link)
            return []


    def _getExtInfo(self, line, info):
        return re.sub("\".*", "", re.sub(".*" + info + "=\"", "", line))


    def _threadParseList(self, link):
        print("Downloading list:", link)
        lines = self._getLinkLines(link)
        if len(lines) > 3 * 1024 * 1024:
            print("\t[\033[91mSIZE\033[0m]", link)
            return
        if lines:
            re.sub("(#EXTINF.*\n)(http.*\n)", self._regExCallback, lines)

    def _regExCallback(self, match):

        all      = match.group(0).strip().split("\n")
        line     = all[0]
        url      = all[1]
        headers  = {
            "User-Agent"    : "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2227.1 Safari/537.36",
            "Connection"    : "close",
            "Cache-Control" : "no-cache"
        }

        logo  = self._getExtInfo(line, "tvg-logo").strip()
        name  = self._getExtInfo(line, "tvg-name").strip()

        name  = name if len(name) and "#" not in name else re.sub(".*,", "", line).strip()
        logo  = logo if validators.url(logo) else "https://78.media.tumblr.com/avatar_98bf3185c980_128.pnj"
        group = ""
        name  = re.sub("(\(.*)|(\[.*)", "", name).strip()

        for i in black_list:
            if i in name.lower() or i in url.lower():
                return

        for attr, value in beg_with.items():
            if name.lower().startswith(attr):
                group = value
                break

        if not group:
            return;

        try:
            if (url in all_urls) or len(all_canais[group][name]) > 3:
                print("\t[\033[93mDUP\033[0m]", name.upper())
                return
        except:
            pass

        if not validators.url(url):
            return

        try:
            sess   = requests.Session()
            req    = sess.get(url, headers=headers, stream=True, timeout=5)
            status = req.status_code
            sess.close()
            del sess, req
        except:
            print("\t[\033[91mOFF\033[0m] (Timeout)", name)
            return

        if ".m3u" not in url and ".ts" not in url:
            return

        if status > 400:
            print("\t[\033[91mOFF\033[0m][", status, "]: ", name)
            return

        print("\t[\033[92mON\033[0m] \033[1;30m(", group, ")\033[0m[", status, "]: ", name)
        self.appendSource(name, logo, group, url)


    def appendSource(self, name, logo, group, url):
        while global_lock.locked():
            time.sleep(30/1000000)
            continue

        global_lock.acquire()

        try:
            name  = name.upper()
            group = group.upper()

            try:
                all_canais[group]
            except KeyError:
                all_canais[group] = {}

            try:
                all_canais[group][name]
            except KeyError:
                all_canais[group][name] = []

            all_canais[group][name].append({
                "logo" : logo,
                "url"  : url
            })
            all_urls.append(url)
        except:
            pass

        global_lock.release()
        return True

    def ksort(self, d):
        return [{k: d[k]} for k in sorted(d.keys())]


if __name__ == "__main__":

    black_list = [
        "bergamota",
        "canales",
        "chile",
        "infantiles",
        "peliculas",
        "ecuador",
        "mexico",
        "calidad",
        "espan",
        "españ",
        "nuestra",
        "camara",
        "câmara",
        "latino",
        "suiza",
        "collage",
        "usa",
        "sur",
        "business",
        "atlanta",
        "sexta",
        "spain",
        "privada",
        "inernacional",
        "classic",
        "fox comedy",
        "fox premium",
        "fox action",
        "fox family",
        "fox cinema",
        "fox 23",
        "fox 29",
        "fox 31",
        "fox 32",
        "fox 5",
        "estados unidos",
        "premiun",
        "manipuladora",
        "interior",
        "no audio",
        "noaudio",
        "radio",
        "mtv rocks",
        "mtv dance",
        "mtv live",
        "mtv hits",
        "japan",
        "russia",
        "uk",
        "fm",
        "internacion",
        "kosova",
        "clipes",
        "band pa",
        "banda max",
        "record mt",
        "nordeste",
        "goias",
        "portugal",
        "água boa",
        "agua boa",
        "record pt",
        "signature",
        "angola",
        "sucesso",
        "bosna",
        "norge",
        "banda",
        "lab",
        "science",
        "/ru/",
        "/en/",
        "tvgeral",
        "akamaihd",
        "tvpremiumhd",
        "futeboltvgratis",
        "cultura:",
        "nasa",
        "turk",
        "espan"
    ]
    beg_with = {
        "sportv"     : "Esportes",
        "fox sport"  : "Esportes",
        "fox sports" : "Esportes",
        "foxsport"   : "Esportes",
        "combate"    : "Esportes",
        "espn"       : "Esportes",
        "premiere"   : "Esportes",

        "telecine"         : "Filmes",
        "megapix"          : "Filmes",
        "hbo"              : "Filmes",
        "studio universal" : "Filmes",
        "space"            : "Filmes",
        "max"              : "Filmes",
        "paramount"        : "Filmes",
        "band sport"       : "Filmes",

        "universal" : "Séries",
        "fox"       : "Séries",
        "fx"        : "Séries",
        "warner"    : "Séries",
        "sony"      : "Séries",
        "syfy"      : "Séries",
        "tnt"       : "Séries",
        "axn"       : "Séries",
        "amc"       : "Séries",
        "a&e"       : "Séries",
        "tbs"       : "Séries",

        "cartoon"        : "Infantis",
        "nick"           : "Infantis",
        "disney"         : "Infantis",
        "gloob"          : "Infantis",
        "natgeokids"     : "Infantis",
        "natgeo kids"    : "Infantis",
        "nat geo kids"   : "Infantis",
        "tooncast"       : "Infantis",
        "discovery kids" : "Infantis",
        "boomerang"      : "Infantis",

        "discovery"           : "Documentários",
        "natgeo"              : "Documentários",
        "national geographic" : "Documentários",
        "animal planet"       : "Documentários",
        "history"             : "Documentários",
        "h2"                  : "Documentários",
        "investigacao"        : "Documentários",
        "investigação"        : "Documentários",
        "home & health"       : "Documentários",
        "home&health"         : "Documentários",
        "h&h"                 : "Documentários",

        "gnt"            : "Variedades",
        "mtv"            : "Variedades",
        "multishow"      : "Variedades",
        "comedy central" : "Variedades",
        "tru tv"         : "Variedades",
        "trutv"          : "Variedades",
        "viva"           : "Variedades",
        "off"            : "Variedades",

        "globo news"   : "Notícias",
        "band news"    : "Notícias",
        # "record news"  : "Notícias",

        "globo sp"  : "Abertos",
        "globo rj"  : "Abertos",
        "globo rbs" : "Abertos",
        "sbt sp"    : "Abertos",
        "sbt rs"    : "Abertos",
        "recordsp"  : "Abertos",
        "band"      : "Abertos",
        "futura"    : "Abertos",
        "cultura"   : "Abertos",

        "bob esponja"       : "24 Horas",
        "scooby doo"        : "24 Horas",
        "dragon ball"       : "24 Horas",
        "dragonball"        : "24 Horas",
        "pokemon"           : "24 Horas",
        "pokémon"           : "24 Horas",
        "south park"        : "24 Horas",
        "southpark"         : "24 Horas",
        "simpsons"          : "24 Horas",
        "os simpsons"       : "24 Horas",
        "chaves"            : "24 Horas",
        "chapolin"          : "24 Horas",
        "naruto"            : "24 Horas",
        "liga da justiça"   : "24 Horas",
        "cavaleiros do zod" : "24 Horas",
        "corrida maluca"    : "24 Horas",
        "apenas um show"    : "24 Horas",
        "ursos sem curso"   : "24 Horas",
        "pernalonga"        : "24 Horas",
        "pica pau"          : "24 Horas",
        "picapau"           : "24 Horas",
        "eu, a patroa"      : "24 Horas",
        "eu a patroa"       : "24 Horas",
        "ben 10"            : "24 Horas",

        # "playboy"  : "x Adultos +18",
        # "hustle"   : "x Adultos +18",
        # "sexyhot"  : "x Adultos +18",
        # "venus"    : "x Adultos +18",
        # "miami tv" : "x Adultos +18",
        # "miamitv"  : "x Adultos +18",
        # "jasmine"  : "x Adultos +18"
    }

    all_urls   = []
    all_names  = {}
    all_canais = {}

    try:
        global_lock = threading.Lock()
        links       = Crawler()
        threads     = []
        now         = datetime.datetime.now()
        dthr        = now.strftime("%d/%m/%Y %H:%M:%S")
        path        = os.path.dirname(os.path.abspath(inspect.stack()[0][1]))
        all         = [
            "https://bit.ly/faustinotv",
            "https://tinyurl.com/CANAISGLAU",
            "https://tinyurl.com/ESPORTEGLAU",
            "https://tinyurl.com/DOCUMENTARIOSGLAU",
            "http://minhalista.live/LANNNN",
            "http://bit.ly/Will-Canais"
        ]

        for i in links.links:
            print("Collecting from", i)
            crawl = getattr(links, i)
            all  += crawl()

        all = list(set(all))

        for link in all:
            t = threading.Thread(target=links._threadParseList, args=(link,))
            t.start()
            threads.append(t)

        for t in threads:
            t.join()
    except (KeyboardInterrupt, SystemExit):
        print("Exiting")
    except Exception as e:
        print("Exception", e)
        pass

    path       = os.path.dirname(os.path.abspath(inspect.stack()[0][1]))
    temp_file  = os.path.join(path, links.file_name_m3u_tmp)
    m3u_file   = os.path.join(path, links.file_name_m3u)
    all_canais = links.ksort(all_canais)

    print("Cleaning temp file")
    if os.path.exists(temp_file):
        os.remove(temp_file)

    print("Creating temp file")
    with open(temp_file, "a+") as file:

        file.write("#EXTM3U\n\n")
        file.write("#PLAYLISTV: pltv-name=\"Bergamota list\" pltv-description=\"Lista de canais muito cítrica :)\" pltv-author=\"Bergamota Inc.\"\n\n")
        # file.write("#EXTINF:-1 tvg-id="" group-title=\"Info\" tvg-logo=\"http://bit.ly/bergamota-logo-small\", Bergamota List\nhttp://0.0.0.0\n\n")
        file.write("#EXTINF:-1 tvg-id="" group-title=\"Info\" tvg-logo=\"http://bit.ly/bergamota-logo-small\", Atualizada em: " + dthr + "\nhttp://0.0.0.0\n\n")
        # file.write("#EXTINF:-1 tvg-id="" group-title=\"Info\" tvg-logo=\"http://bit.ly/bergamota-logo-small\", Ajude a manter a lista\nhttp://0.0.0.0\n\n")
        # file.write("#EXTINF:-1 tvg-id="" group-title=\"Info\" tvg-logo=\"http://bit.ly/bergamota-logo-small\", bit.ly/doar-iptv\nhttp://0.0.0.0\n\n")

        for i in all_canais:
            for j in i:
                ord_canais = links.ksort(i[j])
                for k in ord_canais:
                    for l in k:
                        for m in k:
                            for n in k[m]:
                                file.write("#EXTINF:-1 tvg-name=\"%s\" tvg-logo=\"%s\" group-title=\"%s\",%s\n%s\n\n" % (
                                    m,
                                    n["logo"],
                                    j,
                                    m,
                                    n["url"]
                                ))

    print("Creating m3u file")
    os.rename(temp_file, m3u_file)

    print("Done")

