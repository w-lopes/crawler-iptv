#!/usr/bin/env python3
# -*- coding: Utf-8 -*-
import datetime
import inspect
import os
import re
import requests
import sys
import threading
import validators

class Crawler:
	"""
	Classe para 'crawlear' os sites.
	"""

	links = [
		"listasiptvgratis"
	]

	file_name_m3u     = "bergamota.m3u"
	file_name_m3u_tmp = "bergamota.tmp"

	def listasiptvgratis(self):
		"""
		Crawling para o site 'https://listasiptvgratis.com/'.

		Returns:
			str[]: Links coletados do site
		"""
		content = self._getLinkLines("https://listasiptvgratis.com/", True)
		links   = []
		valids  = [i for j in (range(21, 31), range(84, 94)) for i in j]
		counter = 0
		for line in content:
			if "data-clipboard-text" not in line:
				continue
			counter += 1
			if counter not in valids:
				continue
			links.append(re.sub("\".*", "", re.sub(".*data-clipboard-text=\"", "", line)))
		return links


	def _getLinkLines(self, link, split=False):
		"""
		Retorna o conteudo de uma link splitado por quebra de linha.

		Args:
			link (str): Link para coletar o conteudo

		Returns:
			str[]: Linhas coletadas
		"""
		headers  = {
			"User-Agent" : "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2227.1 Safari/537.36",
			"Connection" : "close"
		}
		try:
			response = requests.get(link, headers=headers, timeout=5)
			if split:
				return re.sub("\r", "", response.text).split("\n")
			else:
				return re.sub("\r", "", response.text)
		except:
			return []


	def _getExtInfo(self, line, info):
		return re.sub("\".*", "", re.sub(".*" + info + "=\"", "", line))


	def _threadParseList(self, link):
		print("Downloading list:", link)
		lines = self._getLinkLines(link)
		if len(lines) > 1 * 1024 * 1024:
			print("\t[\033[91mSIZE\033[0m]", link)
			return
		if lines:
			re.sub("(#EXTINF.*\n)(http.*\n)", self._regExCallback, lines)

	def clearFile(self):
		path = os.path.dirname(os.path.abspath(inspect.stack()[0][1]))
		with open(os.path.join(path, self.file_name_m3u_tmp), "w+") as file:
			file.write("")


	def _regExCallback(self, match):
		global all_urls, all_names, black_list, pre_group

		all      = match.group(0).strip().split("\n")
		line     = all[0]
		url      = all[1]
		headers  = {
			"User-Agent" : "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2227.1 Safari/537.36",
			"Connection" : "close"
		}

		logo  = self._getExtInfo(line, "tvg-logo").strip()
		name  = self._getExtInfo(line, "tvg-name").strip()
		aname = re.sub("\W", "", name)

		if re.search('^[a-zA-Z]{1,4}:', name):
			return

		name  = name if len(name) and "#" not in name else re.sub(".*,", "", line).strip()
		logo  = logo if validators.url(logo) else ""
		group = ""

		for i in black_list:
			if i in name.lower():
				return

		for attr, value in pre_group.items():
			if attr in name.lower():
				group = value
				break

		if not group:
			return;

		if (url in all_urls) or (aname in all_names and all_names[aname] > 2):
			print("\t[\033[93mDUP\033[0m]", name)
			return

		if not validators.url(url):
			return

		try:
			req    = requests.head(url, headers=headers, timeout=3)
			status = req.status_code
		except:
			print("\t[\033[91mOFF\033[0m] (Timeout)", name)
			return

		if ".m3u" not in url and ".ts" not in url:
			return

		if status > 400:
			print("\t[\033[91mOFF\033[0m][", status, "]: ", name)
			return

		all_urls.append(url)
		if (name in all_names):
			all_names[aname] = all_names[aname] + 1
		else:
			all_names[aname] = 1

		print("\t[\033[92mON\033[0m] \033[1;30m(", group, ")\033[0m", name)
		self.appendSourceLine(name, logo, group, url)


	def appendLine(self, line):
		line = line + "\n\n"
		path = os.path.dirname(os.path.abspath(inspect.stack()[0][1]))

		while global_lock.locked():
			continue

		global_lock.acquire()

		with open(os.path.join(path, self.file_name_m3u_tmp), "a+") as file:
			file.write(line)
			file.close()

		global_lock.release()
		return True


	def appendSourceLine(self, name, logo, group, url):
		line = "#EXTINF:-1 tvg-name=\"%s\" tvg-logo=\"%s\" group-title=\"%s\",%s\n%s" % (
			name,
			logo,
			group,
			name,
			url
		)
		return self.appendLine(line)


if __name__ == "__main__":

	black_list = [
		"canales",
		"chile",
		"infantiles",
		"peliculas",
		"ecuador",
		"mexico",
		"calidad",
		"espan",
		"españ",
		"nuestra",
		"camara",
		"câmara",
		"latino",
		"suiza",
		"collage",
		"usa",
		"sur",
		"business",
		"atlanta",
		"sexta",
		"spain",
		"privada",
		"inernacional",
		"cbs",
		"tv",
		"!",
		"-",
		"(",
		")"
	]
	pre_group = {
		"sky"       : "SKY",

		"sportv"    : "Esporte",
		"fox sport" : "Esporte",
		"foxsport"  : "Esporte",
		"combate"   : "Esporte",
		"espn"      : "Esporte",
		"premiere"  : "Esporte",

		"telecine"  : "Filme/Serie",
		"fox"       : "Filme/Serie",
		"warner"    : "Filme/Serie",
		"hbo"       : "Filme/Serie",
		"universal" : "Filme/Serie",

		"cartoon"   : "Infantil",
		"nick"      : "Infantil",
		"disney"    : "Infantil",
		"gloob"     : "Infantil",
		"kids"      : "Infantil",

		"discovery"           : "Documentário",
		"natgeo"              : "Documentário",
		"national geographic" : "Documentário",
		"animal planet"       : "Documentário",
		"history"             : "Documentário",

		"gnt"            : "Variedades",
		"mtv"            : "Variedades",
		"multishow"      : "Variedades",
		"comedy central" : "Variedades",

		"news" : "Notícias",

		"globo"  : "Aberto",
		"sbt"    : "Aberto",
		"record" : "Aberto",
		"band"   : "Aberto",

		"xxx"      : "Adulto",
		"adult"    : "Adulto",
		"playboy"  : "Adulto",
		"sex"      : "Adulto",
		"hot"      : "Adulto",
		"venus"    : "Adulto",
		"miami tv" : "Adulto",
		"miamitv"  : "Adulto"
	}

	all_urls  = []
	all_names = {}

	try:
		global_lock = threading.Lock()
		links       = Crawler()
		threads     = []
		now         = datetime.datetime.now()
		dthr        = now.strftime("%d/%m/%Y %H:%M:%S")
		path        = os.path.dirname(os.path.abspath(inspect.stack()[0][1]))
		all         = ["https://bit.ly/faustinotv"]

		links.clearFile()

		links.appendLine("#EXTM3U")
		links.appendLine("#PLAYLISTV: pltv-name=\"Bergamota list\" pltv-description=\"Lista de canais muito cítrica :)\" pltv-author=\"Bergamota Inc.\"")
		links.appendLine("#EXTINF:-1 tvg-id="" group-title=\"Info\", Atualizada em: " + dthr)

		for i in links.links:
			print("Collecting from", i)
			crawl = getattr(links, i)
			all  += crawl()

		all = list(set(all))

		for link in all:
			t = threading.Thread(target=links._threadParseList, args=(link,))
			t.start()
			threads.append(t)

		for t in threads:
			t.join()

		os.rename(os.path.join(path, links.file_name_m3u_tmp), os.path.join(path, links.file_name_m3u))
		print("Done")
	except (KeyboardInterrupt, SystemExit):
		os.rename(os.path.join(path, links.file_name_m3u_tmp), os.path.join(path, links.file_name_m3u))
		print("Exiting")
		sys.exit(0)
	except Exception as e:
		print("Exception", e)
		pass
